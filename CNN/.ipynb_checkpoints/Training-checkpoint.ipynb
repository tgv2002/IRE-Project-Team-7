{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d09f5c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required installations\n",
    "# !pip cache purge\n",
    "# !python3 -m pip install -U scikit-learn scipy\n",
    "# !pip install nltk\n",
    "# !pip install keras\n",
    "# !pip install gensim\n",
    "# !pip install matplotlib\n",
    "# !pip install pandas\n",
    "# !pip install scikit-learn\n",
    "# !pip install nltk\n",
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "equal-taylor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, GlobalAveragePooling2D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "artificial-phenomenon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning data\n",
    "def clean_post(post):\n",
    "    post = post.lower()\n",
    "    post = re.sub(r\"\\n\", \" \", post)\n",
    "    post = re.sub(\"[\\<\\[].*?[\\>\\]]\", \" \", post)\n",
    "    post = re.sub(r\"[^a-z ]\", \" \", post)\n",
    "    post = re.sub(r\"\\b\\w{1,3}\\b\", \" \", post)\n",
    "    return \" \".join([x for x in post.split() if x not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e15568e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different techniques for tackling class imbalance\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss, ClusterCentroids\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "\n",
    "def balance_data(x, y, _type):\n",
    "    if _type == 0:\n",
    "        ros = RandomOverSampler(random_state=42)\n",
    "        return ros.fit_resample(x, y)\n",
    "    elif _type == 1:\n",
    "        rus = RandomUnderSampler(random_state=42, replacement=True)\n",
    "        return rus.fit_resample(x, y)\n",
    "    elif _type == 2:\n",
    "        smote = SMOTE()\n",
    "        return smote.fit_resample(x, y)\n",
    "    elif _type == 3:\n",
    "        nm = NearMiss()\n",
    "        return nm.fit_resample(x, y)\n",
    "    elif _type == 5:\n",
    "        cc = ClusterCentroids()\n",
    "        return cc.fit_resample(x, y)\n",
    "    elif _type == 6:\n",
    "        tl = TomekLinks()\n",
    "        return tl.fit_resample(x, y)\n",
    "    return x, y\n",
    "    # Another technique is penalizing the algo with class_weight=balanced, using stratified cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "daily-office",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                mental_disorder                           \n",
      "                          count unique            top freq\n",
      "mental_disorder                                           \n",
      "EDAnonymous                  41      1    EDAnonymous   41\n",
      "addiction                    13      1      addiction   13\n",
      "adhd                        111      1           adhd  111\n",
      "alcoholism                   19      1     alcoholism   19\n",
      "anxiety                     181      1        anxiety  181\n",
      "autism                       22      1         autism   22\n",
      "bipolarreddit                18      1  bipolarreddit   18\n",
      "bpd                          63      1            bpd   63\n",
      "depression                  336      1     depression  336\n",
      "healthanxiety                22      1  healthanxiety   22\n",
      "lonely                       69      1         lonely   69\n",
      "ptsd                         30      1           ptsd   30\n",
      "schizophrenia                17      1  schizophrenia   17\n",
      "socialanxiety                60      1  socialanxiety   60\n",
      "suicidewatch                198      1   suicidewatch  198\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('../split_data/train_and_valid.csv')\n",
    "data = shuffle(data)\n",
    "\n",
    "# Class split stats\n",
    "print(data.groupby(['mental_disorder'])[['mental_disorder']].describe())\n",
    "X = data['post'].apply(lambda post: clean_post(post))\n",
    "label_encoder = LabelEncoder()\n",
    "y1 = label_encoder.fit_transform(np.array(data['mental_disorder']))\n",
    "y = to_categorical(y1)\n",
    "\n",
    "# 70-15-15 split (test data is unseen)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.176, random_state=321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "expired-bikini",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens found: 8044\n"
     ]
    }
   ],
   "source": [
    "# Using keras tokenizer on text for pre-processing\n",
    "MAX_WORDS_LIMIT = 30000\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS_LIMIT, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'', lower=True)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "with open('../models/tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "word_index = tokenizer.word_index\n",
    "print(f'Unique tokens found: {len(word_index)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "offshore-discipline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X train tensor: (988, 200), X validation tensor: (212, 200)\n",
      "Shape of y train tensor: (988, 15), y validation tensor: (212, 15)\n"
     ]
    }
   ],
   "source": [
    "# Convert  texts to sequence of integers\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_valid = tokenizer.texts_to_sequences(X_valid)\n",
    "\n",
    "# Limit size of train/validation/test sequences to 200 and pad the sequence\n",
    "X_train = pad_sequences(sequences_train, maxlen=200)\n",
    "X_valid = pad_sequences(sequences_valid, maxlen=X_train.shape[1])\n",
    "print(f'Shape of X train tensor: {X_train.shape}, X validation tensor: {X_valid.shape}')\n",
    "\n",
    "# Convert target to array\n",
    "y_train, y_valid = np.asarray(y_train), np.asarray(y_valid)\n",
    "print(f'Shape of y train tensor: {y_train.shape}, y validation tensor: {y_valid.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "respected-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating word embeddings via pre-trained word2vec model\n",
    "WORD_EMBEDDING_DIM = 300\n",
    "word_vectors = KeyedVectors.load_word2vec_format('../reddit_mental_health_dataset/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "embeddings_matrix = np.zeros((MAX_WORDS_LIMIT, WORD_EMBEDDING_DIM))\n",
    "\n",
    "# Computing embeddings matrix and embedding layer\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_WORDS_LIMIT:\n",
    "        break\n",
    "    try:\n",
    "        embeddings_matrix[i] = word_vectors[word]\n",
    "    except:\n",
    "        embeddings_matrix[i] = np.zeros(WORD_EMBEDDING_DIM)\n",
    "embedding_layer = Embedding(MAX_WORDS_LIMIT, WORD_EMBEDDING_DIM, weights=[embeddings_matrix], trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f35bc6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Approach - 1\n",
    "# # Parameters\n",
    "# sequence_length = X_train.shape[1]\n",
    "# filter_sizes = [3, 4]\n",
    "# num_filters = 128\n",
    "# drop = 0.4\n",
    "\n",
    "# # Obtaining embeddings based on input sequence\n",
    "# inputs = Input(shape=(sequence_length,))\n",
    "# embedding = embedding_layer(inputs)\n",
    "# reshape = Reshape((sequence_length, WORD_EMBEDDING_DIM, 1))(embedding)\n",
    "\n",
    "# # Creating convolutional and maxpool layers\n",
    "# conv_layers, maxpool_layers = [], []\n",
    "# for i in range(2):\n",
    "#     conv_layers.append(Conv2D(num_filters, (filter_sizes[i], WORD_EMBEDDING_DIM), activation='relu', \n",
    "#                               kernel_regularizer=regularizers.l2(0.01))(reshape))\n",
    "#     maxpool_layers.append(MaxPooling2D((sequence_length - filter_sizes[i] + 1, 1), strides=(1, 1))(conv_layers[i]))\n",
    "\n",
    "# # Constructing the complete network and creating model\n",
    "# merged_tensor = concatenate(maxpool_layers, axis=1)\n",
    "# flatten = Flatten()(merged_tensor)\n",
    "# reshape = Reshape((2*num_filters,))(flatten)\n",
    "# dropout = Dropout(drop)(flatten)\n",
    "# conc = Dense(60)(flatten)\n",
    "# output = Dense(units=15, activation='softmax')(conc)\n",
    "# model = Model(inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cleared-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Approach - 2\n",
    "# # Parameters\n",
    "# sequence_length = X_train.shape[1]\n",
    "# num_filters = 128\n",
    "# drop = 0.25\n",
    "\n",
    "# # Obtaining embeddings based on input sequence\n",
    "# inputs = Input(shape=(sequence_length,))\n",
    "# embedding = embedding_layer(inputs)\n",
    "# reshape = Reshape((sequence_length, WORD_EMBEDDING_DIM, 1))(embedding)\n",
    "\n",
    "# # Constructing the complete network and creating model\n",
    "# conv = Conv2D(num_filters, (5, WORD_EMBEDDING_DIM), activation='relu', kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "# dropout = Dropout(drop)(conv)\n",
    "# pool = MaxPooling2D((sequence_length - 4, 1), strides=(1, 1))(dropout)\n",
    "# flatten = GlobalAveragePooling2D()(pool)\n",
    "# dropout = Dropout(drop)(flatten)\n",
    "# conc = Dense(60)(flatten)\n",
    "# output = Dense(units=15, activation='softmax')(conc)\n",
    "# model = Model(inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eb8ca12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 3\n",
    "# Parameters\n",
    "sequence_length = X_train.shape[1]\n",
    "num_filters = 250\n",
    "drop = 0.25\n",
    "\n",
    "# Obtaining embeddings based on input sequence\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding = embedding_layer(inputs)\n",
    "\n",
    "# Constructing the complete network and creating model\n",
    "conv = Conv1D(num_filters, 5, activation='relu', kernel_regularizer=regularizers.l2(0.01))(embedding)\n",
    "dropout = Dropout(drop)(conv)\n",
    "pool = GlobalMaxPooling1D()(dropout)\n",
    "conc = Dense(100)(pool)\n",
    "dropout2 = Dropout(drop)(conc)\n",
    "output = Dense(units=15, activation='softmax')(dropout2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "hazardous-omaha",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##############################################################################################################\n",
      "\n",
      "With sampling type: 1\n",
      "\n",
      "\n",
      "Epoch 1/7\n",
      "1/1 - 1s - loss: 5.5849 - accuracy: 0.0833 - val_loss: 5.4743 - val_accuracy: 0.0047\n",
      "Epoch 2/7\n",
      "1/1 - 1s - loss: 5.2100 - accuracy: 0.1444 - val_loss: 5.4732 - val_accuracy: 0.0189\n",
      "Epoch 3/7\n",
      "1/1 - 1s - loss: 4.9019 - accuracy: 0.4222 - val_loss: 5.3973 - val_accuracy: 0.0425\n",
      "Epoch 4/7\n",
      "1/1 - 1s - loss: 4.5558 - accuracy: 0.6444 - val_loss: 5.3047 - val_accuracy: 0.0425\n",
      "Epoch 5/7\n",
      "1/1 - 1s - loss: 4.3421 - accuracy: 0.7333 - val_loss: 5.2157 - val_accuracy: 0.0802\n",
      "Epoch 6/7\n",
      "1/1 - 1s - loss: 4.0681 - accuracy: 0.8667 - val_loss: 5.1345 - val_accuracy: 0.1462\n",
      "Epoch 7/7\n",
      "1/1 - 1s - loss: 3.8448 - accuracy: 0.9556 - val_loss: 5.0597 - val_accuracy: 0.1745\n",
      "INFO:tensorflow:Assets written to: ../models/CNN_model_1/assets\n",
      "##############################################################################################################\n",
      "\n",
      "With sampling type: 3\n",
      "\n",
      "\n",
      "Epoch 1/7\n",
      "1/1 - 1s - loss: 4.5449 - accuracy: 0.4389 - val_loss: 5.0712 - val_accuracy: 0.1321\n",
      "Epoch 2/7\n",
      "1/1 - 1s - loss: 4.2795 - accuracy: 0.6444 - val_loss: 5.0104 - val_accuracy: 0.1462\n",
      "Epoch 3/7\n",
      "1/1 - 1s - loss: 4.0107 - accuracy: 0.7889 - val_loss: 4.9297 - val_accuracy: 0.1651\n",
      "Epoch 4/7\n",
      "1/1 - 1s - loss: 3.8107 - accuracy: 0.9056 - val_loss: 4.8655 - val_accuracy: 0.1887\n",
      "Epoch 5/7\n",
      "1/1 - 1s - loss: 3.5756 - accuracy: 0.9556 - val_loss: 4.8094 - val_accuracy: 0.1934\n",
      "Epoch 6/7\n",
      "1/1 - 1s - loss: 3.3574 - accuracy: 0.9833 - val_loss: 4.7523 - val_accuracy: 0.2264\n",
      "Epoch 7/7\n",
      "1/1 - 1s - loss: 3.1932 - accuracy: 1.0000 - val_loss: 4.6880 - val_accuracy: 0.2689\n",
      "INFO:tensorflow:Assets written to: ../models/CNN_model_3/assets\n",
      "##############################################################################################################\n",
      "\n",
      "With sampling type: 5\n",
      "\n",
      "\n",
      "Epoch 1/7\n",
      "1/1 - 2s - loss: 4.1983 - accuracy: 0.3889 - val_loss: 4.3893 - val_accuracy: 0.3255\n",
      "Epoch 2/7\n",
      "1/1 - 1s - loss: 3.9010 - accuracy: 0.5111 - val_loss: 4.3667 - val_accuracy: 0.3208\n",
      "Epoch 3/7\n",
      "1/1 - 1s - loss: 3.6414 - accuracy: 0.7000 - val_loss: 4.4204 - val_accuracy: 0.2642\n",
      "Epoch 4/7\n",
      "1/1 - 1s - loss: 3.3597 - accuracy: 0.9167 - val_loss: 4.4499 - val_accuracy: 0.2453\n",
      "Epoch 5/7\n",
      "1/1 - 1s - loss: 3.1680 - accuracy: 0.9667 - val_loss: 4.4335 - val_accuracy: 0.2594\n",
      "Epoch 6/7\n",
      "1/1 - 1s - loss: 2.9846 - accuracy: 0.9944 - val_loss: 4.3749 - val_accuracy: 0.2830\n",
      "Epoch 7/7\n",
      "1/1 - 1s - loss: 2.8186 - accuracy: 1.0000 - val_loss: 4.2982 - val_accuracy: 0.3255\n",
      "INFO:tensorflow:Assets written to: ../models/CNN_model_5/assets\n",
      "##############################################################################################################\n",
      "\n",
      "With sampling type: 6\n",
      "\n",
      "\n",
      "Epoch 1/7\n",
      "2/2 - 3s - loss: 3.6060 - accuracy: 0.5272 - val_loss: 3.8528 - val_accuracy: 0.3113\n",
      "Epoch 2/7\n",
      "2/2 - 3s - loss: 3.2761 - accuracy: 0.4906 - val_loss: 3.7174 - val_accuracy: 0.4198\n",
      "Epoch 3/7\n",
      "2/2 - 3s - loss: 3.0579 - accuracy: 0.6160 - val_loss: 3.6340 - val_accuracy: 0.3774\n",
      "Epoch 4/7\n",
      "2/2 - 3s - loss: 2.8291 - accuracy: 0.7059 - val_loss: 3.5183 - val_accuracy: 0.4387\n",
      "Epoch 5/7\n",
      "2/2 - 2s - loss: 2.6537 - accuracy: 0.8213 - val_loss: 3.3977 - val_accuracy: 0.4670\n",
      "Epoch 6/7\n",
      "2/2 - 3s - loss: 2.4661 - accuracy: 0.8313 - val_loss: 3.3061 - val_accuracy: 0.4245\n",
      "Epoch 7/7\n",
      "2/2 - 2s - loss: 2.3232 - accuracy: 0.8191 - val_loss: 3.2136 - val_accuracy: 0.4764\n",
      "INFO:tensorflow:Assets written to: ../models/CNN_model_6/assets\n",
      "##############################################################################################################\n",
      "\n",
      "Without any oversampling/undersampling\n",
      "\n",
      "\n",
      "Epoch 1/7\n",
      "2/2 - 4s - loss: 2.3159 - accuracy: 0.7966 - val_loss: 3.1391 - val_accuracy: 0.4151\n",
      "Epoch 2/7\n",
      "2/2 - 3s - loss: 2.1648 - accuracy: 0.7945 - val_loss: 3.0681 - val_accuracy: 0.4434\n",
      "Epoch 3/7\n",
      "2/2 - 3s - loss: 2.0112 - accuracy: 0.8856 - val_loss: 2.9449 - val_accuracy: 0.4906\n",
      "Epoch 4/7\n",
      "2/2 - 3s - loss: 1.8721 - accuracy: 0.9231 - val_loss: 2.8993 - val_accuracy: 0.4151\n",
      "Epoch 5/7\n",
      "2/2 - 3s - loss: 1.7727 - accuracy: 0.9109 - val_loss: 2.8203 - val_accuracy: 0.4528\n",
      "Epoch 6/7\n",
      "2/2 - 3s - loss: 1.6542 - accuracy: 0.9534 - val_loss: 2.7713 - val_accuracy: 0.5047\n",
      "Epoch 7/7\n",
      "2/2 - 3s - loss: 1.5635 - accuracy: 0.9798 - val_loss: 2.6833 - val_accuracy: 0.5047\n",
      "INFO:tensorflow:Assets written to: ../models/CNN_model_-1/assets\n",
      "##############################################################################################################\n",
      "\n",
      "With sampling type: 0\n",
      "\n",
      "\n",
      "Epoch 1/7\n",
      "7/7 - 12s - loss: 1.3848 - accuracy: 0.9860 - val_loss: 2.5451 - val_accuracy: 0.5425\n",
      "Epoch 2/7\n",
      "7/7 - 11s - loss: 1.0566 - accuracy: 0.9950 - val_loss: 2.3550 - val_accuracy: 0.5330\n",
      "Epoch 3/7\n",
      "7/7 - 11s - loss: 0.8251 - accuracy: 0.9986 - val_loss: 2.1225 - val_accuracy: 0.5425\n",
      "Epoch 4/7\n",
      "7/7 - 11s - loss: 0.6493 - accuracy: 0.9990 - val_loss: 1.9727 - val_accuracy: 0.5660\n",
      "Epoch 5/7\n",
      "7/7 - 11s - loss: 0.5176 - accuracy: 0.9988 - val_loss: 1.8724 - val_accuracy: 0.5566\n",
      "Epoch 6/7\n",
      "7/7 - 11s - loss: 0.4228 - accuracy: 0.9995 - val_loss: 1.7910 - val_accuracy: 0.5283\n",
      "Epoch 7/7\n",
      "7/7 - 11s - loss: 0.3573 - accuracy: 0.9993 - val_loss: 1.7130 - val_accuracy: 0.5094\n",
      "INFO:tensorflow:Assets written to: ../models/CNN_model_0/assets\n",
      "##############################################################################################################\n",
      "\n",
      "With sampling type: 2\n",
      "\n",
      "\n",
      "Epoch 1/7\n",
      "7/7 - 11s - loss: 2.8463 - accuracy: 0.2305 - val_loss: 2.3929 - val_accuracy: 0.3962\n",
      "Epoch 2/7\n",
      "7/7 - 11s - loss: 2.3292 - accuracy: 0.3321 - val_loss: 2.2933 - val_accuracy: 0.4670\n",
      "Epoch 3/7\n",
      "7/7 - 11s - loss: 2.1069 - accuracy: 0.4755 - val_loss: 2.5156 - val_accuracy: 0.3774\n",
      "Epoch 4/7\n",
      "7/7 - 11s - loss: 1.9522 - accuracy: 0.6100 - val_loss: 2.3103 - val_accuracy: 0.4198\n",
      "Epoch 5/7\n",
      "7/7 - 11s - loss: 1.8035 - accuracy: 0.6914 - val_loss: 2.3846 - val_accuracy: 0.4387\n",
      "Epoch 6/7\n",
      "7/7 - 11s - loss: 1.6538 - accuracy: 0.7840 - val_loss: 2.3888 - val_accuracy: 0.4104\n",
      "Epoch 7/7\n",
      "7/7 - 11s - loss: 1.5183 - accuracy: 0.8557 - val_loss: 2.4428 - val_accuracy: 0.3726\n",
      "INFO:tensorflow:Assets written to: ../models/CNN_model_2/assets\n"
     ]
    }
   ],
   "source": [
    "# Fitting Model to the data\n",
    "X_tr, y_tr = X_train, y_train\n",
    "for _type in [1, 3, 5, 6, -1, 0, 2]:\n",
    "    model = Model(inputs, output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=1e-3), metrics=[\"accuracy\"])\n",
    "    print('#'*110)\n",
    "    print()\n",
    "    if _type == -1:\n",
    "        print('Without any oversampling/undersampling')\n",
    "    else:\n",
    "        print(f'With sampling type: {_type}')\n",
    "    print()\n",
    "    print()\n",
    "    X_train, y_train = balance_data(X_tr, y_tr, _type)\n",
    "    hist_adam = model.fit(X_train, y_train, batch_size=600, epochs=7, verbose=2, \n",
    "                          validation_data=(X_valid, y_valid))\n",
    "    # Saving model\n",
    "    model.save(f'../models/CNN_model_{_type}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc138058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
