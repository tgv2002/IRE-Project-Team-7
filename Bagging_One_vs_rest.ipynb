{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bagging One vs rest.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1N9bc-Cf_m7"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import re\n",
        "import nltk\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from nltk.corpus import stopwords\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models.keyedvectors import KeyedVectors"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTEZ3LAngLCv"
      },
      "source": [
        "def get_metrics(y_true, y_pred):\n",
        "    result1 = classification_report(y_true, y_pred)\n",
        "    print('Classification Report: ', result1)\n",
        "    result2 = accuracy_score(y_true, y_pred)\n",
        "    print('Accuracy: ', result2, \"\\n\\n\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIvEjix7gQRj",
        "outputId": "1fb37836-6650-4b05-a81c-b40f99a853fe"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX9H85mEgRn8"
      },
      "source": [
        " stop_words = set(stopwords.words('english'))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LA6HfIqfgTNQ"
      },
      "source": [
        "def clean_post(post):\n",
        "  post = post.lower()\n",
        "  post = re.sub(r\"\\n\", \" \", post)\n",
        "  post = re.sub(\"[\\<\\[].*?[\\>\\]]\", \" \", post)\n",
        "  post = re.sub(r\"[^a-z ]\", \" \", post)\n",
        "  post = re.sub(r\"\\b\\w{1,3}\\b\", \" \", post)\n",
        "  return \" \".join([x for x in post.split() if x not in stop_words])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71AR95q8gUqZ"
      },
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss, ClusterCentroids\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
        "from imblearn.combine import SMOTETomek\n",
        "\n",
        "def balance_data(x, y, _type):\n",
        "  if _type == 0:\n",
        "    ros = RandomOverSampler(random_state=42)\n",
        "    return ros.fit_resample(x, y)\n",
        "  elif _type == 1:\n",
        "    rus = RandomUnderSampler(random_state=42, replacement=True)\n",
        "    return rus.fit_resample(x, y)\n",
        "  elif _type == 2:\n",
        "    smote = SMOTE()\n",
        "    return smote.fit_resample(x, y)\n",
        "  elif _type == 3:\n",
        "    nm = NearMiss()\n",
        "    return nm.fit_resample(x, y)\n",
        "  elif _type == 4:\n",
        "    smt = SMOTETomek(ratio='auto')\n",
        "    return smt.fit_resample(x, y)\n",
        "  elif _type == 5:\n",
        "    cc = ClusterCentroids()\n",
        "    return cc.fit_resample(x, y)\n",
        "  elif _type == 6:\n",
        "    tl = TomekLinks()\n",
        "    return tl.fit_resample(x, y)\n",
        "  # default\n",
        "  smote = SMOTE()\n",
        "  return smote.fit_resample(x, y)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFXVnznpgWKc",
        "outputId": "c18dc80e-3167-4c56-f9b4-185c3a84697e"
      },
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/reddit_dataset.csv')\n",
        "data = shuffle(data)\n",
        "\n",
        "# Class split stats\n",
        "print(data.groupby(['mental_disorder'])[['mental_disorder']].describe())\n",
        "x = data['post'].apply(lambda post: clean_post(post))\n",
        "\n",
        "# Vectorizing text data\n",
        "count_vect = CountVectorizer()\n",
        "X_counts = count_vect.fit_transform(x)\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X = tfidf_transformer.fit_transform(X_counts)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                mental_disorder                              \n",
            "                          count unique            top    freq\n",
            "mental_disorder                                              \n",
            "EDAnonymous               14577      1    EDAnonymous   14577\n",
            "addiction                  7641      1      addiction    7641\n",
            "adhd                      45631      1           adhd   45631\n",
            "alcoholism                 5911      1     alcoholism    5911\n",
            "anxiety                   57671      1        anxiety   57671\n",
            "autism                     8869      1         autism    8869\n",
            "bipolarreddit              5780      1  bipolarreddit    5780\n",
            "bpd                       24294      1            bpd   24294\n",
            "depression               117331      1     depression  117331\n",
            "healthanxiety              8648      1  healthanxiety    8648\n",
            "lonely                    23635      1         lonely   23635\n",
            "ptsd                       8643      1           ptsd    8643\n",
            "schizophrenia              8712      1  schizophrenia    8712\n",
            "socialanxiety             22996      1  socialanxiety   22996\n",
            "suicidewatch              66161      1   suicidewatch   66161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-223yjJBgXqP"
      },
      "source": [
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(np.array(data['mental_disorder']))\n",
        "# y = to_categorical(y1)\n",
        "\n",
        "# 60-20-20 split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=321)\n",
        "X_test, X_valid, y_test, y_valid = train_test_split(X_test, y_test, test_size=0.5, random_state=321)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2ttwX1wge7y",
        "outputId": "af31ae3d-a027-4fa0-a48b-d776e0621f87"
      },
      "source": [
        "import gc\n",
        "\n",
        "X_tr, y_tr = X_train, y_train\n",
        "\n",
        "for _type in [1, 3, 6, -1, 0, 2]:\n",
        "  print('#'*110)\n",
        "  print()\n",
        "  if _type == -1:\n",
        "    print('Without any undersampling/oversampling')\n",
        "  else:\n",
        "    print(f'With sampling type: {_type}')\n",
        "  model = BaggingClassifier(base_estimator=LogisticRegression())\n",
        "  X_train, y_train = balance_data(X_tr, y_tr, _type)\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  y_train_pred = model.predict(X_train)\n",
        "  print()\n",
        "  print(\"For training set\")\n",
        "  print()\n",
        "  get_metrics(y_train, y_train_pred)\n",
        "\n",
        "  y_valid_pred = model.predict(X_valid)\n",
        "  print()\n",
        "  print(\"For validation set\")\n",
        "  print()\n",
        "  get_metrics(y_valid, y_valid_pred)\n",
        "\n",
        "  y_test_pred = model.predict(X_test)\n",
        "  print()\n",
        "  print(\"For test set\")\n",
        "  print()\n",
        "  get_metrics(y_test, y_test_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##############################################################################################################\n",
            "\n",
            "With sampling type: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxJyapqrhB9M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}