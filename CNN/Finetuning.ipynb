{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09f5c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required installations\n",
    "# !pip cache purge\n",
    "# !python3 -m pip install -U scikit-learn scipy\n",
    "# !pip install nltk\n",
    "# !pip install keras\n",
    "# !pip install gensim\n",
    "# !pip install matplotlib\n",
    "# !pip install pandas\n",
    "# !pip install scikit-learn\n",
    "# !pip install nltk\n",
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-taylor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, GlobalAveragePooling2D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPooling2D, Dropout,concatenate\n",
    "from keras.layers.core import Reshape, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-phenomenon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaning data\n",
    "def clean_post(post):\n",
    "    post = post.lower()\n",
    "    post = re.sub(r\"\\n\", \" \", post)\n",
    "    post = re.sub(\"[\\<\\[].*?[\\>\\]]\", \" \", post)\n",
    "    post = re.sub(r\"[^a-z ]\", \" \", post)\n",
    "    post = re.sub(r\"\\b\\w{1,3}\\b\", \" \", post)\n",
    "    return \" \".join([x for x in post.split() if x not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15568e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different techniques for tackling class imbalance\n",
    "from imblearn.under_sampling import RandomUnderSampler, TomekLinks, NearMiss, ClusterCentroids\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "\n",
    "def balance_data(x, y, _type):\n",
    "    if _type == 0:\n",
    "        ros = RandomOverSampler(random_state=42)\n",
    "        return ros.fit_resample(x, y)\n",
    "    elif _type == 1:\n",
    "        rus = RandomUnderSampler(random_state=42, replacement=True)\n",
    "        return rus.fit_resample(x, y)\n",
    "    elif _type == 2:\n",
    "        smote = SMOTE()\n",
    "        return smote.fit_resample(x, y)\n",
    "    elif _type == 3:\n",
    "        nm = NearMiss()\n",
    "        return nm.fit_resample(x, y)\n",
    "    elif _type == 5:\n",
    "        cc = ClusterCentroids()\n",
    "        return cc.fit_resample(x, y)\n",
    "    elif _type == 6:\n",
    "        tl = TomekLinks()\n",
    "        return tl.fit_resample(x, y)\n",
    "    return x, y\n",
    "    # Another technique is penalizing the algo with class_weight=balanced, using stratified cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daily-office",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('../reddit_mental_health_dataset/reddit_dataset.csv')\n",
    "data = shuffle(data)\n",
    "# data = data[:500]\n",
    "\n",
    "# Class split stats\n",
    "print(data.groupby(['mental_disorder'])[['mental_disorder']].describe())\n",
    "X = data['post'].apply(lambda post: clean_post(post))\n",
    "label_encoder = LabelEncoder()\n",
    "y1 = label_encoder.fit_transform(np.array(data['mental_disorder']))\n",
    "y = to_categorical(y1)\n",
    "\n",
    "# 60-20-20 split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=321)\n",
    "X_test, X_valid, y_test, y_valid = train_test_split(X_test, y_test, test_size=0.5, random_state=321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-bikini",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using keras tokenizer on text for pre-processing\n",
    "MAX_WORDS_LIMIT = 30000\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS_LIMIT, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n\\'', lower=True)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "word_index = tokenizer.word_index\n",
    "print(f'Unique tokens found: {len(word_index)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-discipline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert  texts to sequence of integers\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_valid = tokenizer.texts_to_sequences(X_valid)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Limit size of train/validation/test sequences to 200 and pad the sequence\n",
    "X_train = pad_sequences(sequences_train, maxlen=200)\n",
    "X_valid = pad_sequences(sequences_valid, maxlen=X_train.shape[1])\n",
    "X_test = pad_sequences(sequences_test, maxlen=X_train.shape[1])\n",
    "print(f'Shape of X train tensor: {X_train.shape}, X validation tensor: {X_valid.shape}, X test tensor: {X_test.shape}')\n",
    "\n",
    "# Convert target to array\n",
    "y_train, y_valid, y_test = np.asarray(y_train), np.asarray(y_valid), np.asarray(y_test)\n",
    "print(f'Shape of y train tensor: {y_train.shape}, y validation tensor: {y_valid.shape}, y test tensor: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-belize",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating word embeddings via pre-trained word2vec model\n",
    "WORD_EMBEDDING_DIM = 300\n",
    "word_vectors = KeyedVectors.load_word2vec_format('../reddit_mental_health_dataset/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "embeddings_matrix = np.zeros((MAX_WORDS_LIMIT, WORD_EMBEDDING_DIM))\n",
    "\n",
    "# Computing embeddings matrix and embedding layer\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_WORDS_LIMIT:\n",
    "        break\n",
    "    try:\n",
    "        embeddings_matrix[i] = word_vectors[word]\n",
    "    except:\n",
    "        embeddings_matrix[i] = np.zeros(WORD_EMBEDDING_DIM)\n",
    "embedding_layer = Embedding(MAX_WORDS_LIMIT, WORD_EMBEDDING_DIM, weights=[embeddings_matrix], trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35bc6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Approach - 1\n",
    "# # Parameters\n",
    "# sequence_length = X_train.shape[1]\n",
    "# filter_sizes = [3, 4]\n",
    "# num_filters = 128\n",
    "# drop = 0.4\n",
    "\n",
    "# # Obtaining embeddings based on input sequence\n",
    "# inputs = Input(shape=(sequence_length,))\n",
    "# embedding = embedding_layer(inputs)\n",
    "# reshape = Reshape((sequence_length, WORD_EMBEDDING_DIM, 1))(embedding)\n",
    "\n",
    "# # Creating convolutional and maxpool layers\n",
    "# conv_layers, maxpool_layers = [], []\n",
    "# for i in range(2):\n",
    "#     conv_layers.append(Conv2D(num_filters, (filter_sizes[i], WORD_EMBEDDING_DIM), activation='relu', \n",
    "#                               kernel_regularizer=regularizers.l2(0.01))(reshape))\n",
    "#     maxpool_layers.append(MaxPooling2D((sequence_length - filter_sizes[i] + 1, 1), strides=(1, 1))(conv_layers[i]))\n",
    "\n",
    "# # Constructing the complete network and creating model\n",
    "# merged_tensor = concatenate(maxpool_layers, axis=1)\n",
    "# flatten = Flatten()(merged_tensor)\n",
    "# reshape = Reshape((2*num_filters,))(flatten)\n",
    "# dropout = Dropout(drop)(flatten)\n",
    "# conc = Dense(60)(flatten)\n",
    "# output = Dense(units=15, activation='softmax')(conc)\n",
    "# model = Model(inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Approach - 2\n",
    "# # Parameters\n",
    "# sequence_length = X_train.shape[1]\n",
    "# num_filters = 128\n",
    "# drop = 0.25\n",
    "\n",
    "# # Obtaining embeddings based on input sequence\n",
    "# inputs = Input(shape=(sequence_length,))\n",
    "# embedding = embedding_layer(inputs)\n",
    "# reshape = Reshape((sequence_length, WORD_EMBEDDING_DIM, 1))(embedding)\n",
    "\n",
    "# # Constructing the complete network and creating model\n",
    "# conv = Conv2D(num_filters, (5, WORD_EMBEDDING_DIM), activation='relu', kernel_regularizer=regularizers.l2(0.01))(reshape)\n",
    "# dropout = Dropout(drop)(conv)\n",
    "# pool = MaxPooling2D((sequence_length - 4, 1), strides=(1, 1))(dropout)\n",
    "# flatten = GlobalAveragePooling2D()(pool)\n",
    "# dropout = Dropout(drop)(flatten)\n",
    "# conc = Dense(60)(flatten)\n",
    "# output = Dense(units=15, activation='softmax')(conc)\n",
    "# model = Model(inputs, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8ca12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 3\n",
    "# Parameters\n",
    "sequence_length = X_train.shape[1]\n",
    "num_filters = 250\n",
    "drop = 0.25\n",
    "\n",
    "# Obtaining embeddings based on input sequence\n",
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding = embedding_layer(inputs)\n",
    "\n",
    "# Constructing the complete network and creating model\n",
    "conv = Conv1D(num_filters, 5, activation='relu', kernel_regularizer=regularizers.l2(0.01))(embedding)\n",
    "dropout = Dropout(drop)(conv)\n",
    "pool = GlobalMaxPooling1D()(dropout)\n",
    "conc = Dense(100)(pool)\n",
    "dropout2 = Dropout(drop)(conc)\n",
    "output = Dense(units=15, activation='softmax')(dropout2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d79106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(ytrue, ypred):\n",
    "    y_true = np.argmax(ytrue, axis=1)\n",
    "    y_pred = np.argmax(ypred, axis=1)\n",
    "    result1 = classification_report(y_true, y_pred)\n",
    "    print('Classification Report: ', result1)\n",
    "    result2 = accuracy_score(y_true, y_pred)\n",
    "    print('Accuracy: ', result2, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-omaha",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting Model to the data\n",
    "X_tr, y_tr = X_train, y_train\n",
    "for _type in [1, 3, 5, 6, -1, 0, 2]:\n",
    "    model = Model(inputs, output)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=1e-3), metrics=[\"accuracy\"])\n",
    "    print('#'*110)\n",
    "    print()\n",
    "    if _type == -1:\n",
    "        print('Without any oversampling/undersampling')\n",
    "    else:\n",
    "        print(f'With sampling type: {_type}')\n",
    "    print()\n",
    "    print()\n",
    "    X_train, y_train = balance_data(X_tr, y_tr, _type)\n",
    "    hist_adam = model.fit(X_train, y_train, batch_size=600, epochs=7, verbose=2, \n",
    "                          validation_data=(X_valid, y_valid))\n",
    "    # Predict on train, val and test datasets\n",
    "    pred_train = model.predict(X_train)\n",
    "    print()\n",
    "    print(\"For training set\")\n",
    "    print()\n",
    "    get_metrics(y_train, pred_train)\n",
    "    pred_valid = model.predict(X_valid)\n",
    "    print()\n",
    "    print(\"For validation set\")\n",
    "    print()\n",
    "    get_metrics(y_valid, pred_valid)\n",
    "    pred_test = model.predict(X_test)\n",
    "    print()\n",
    "    print(\"For test set\")\n",
    "    print()\n",
    "    get_metrics(y_test, pred_test)\n",
    "    print()\n",
    "    print()\n",
    "    print('#'*110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc138058",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
